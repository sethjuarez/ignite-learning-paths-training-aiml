{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.hyperdrive import *\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Workspace, Datastore, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "To access an Azure ML Workspace, you will need to import the AML library and the following information:\n",
    "* A name for your workspace (in our example - `hal`)\n",
    "* Your subscription id (can be obtained by running `az account list`)\n",
    "* The resource group name (in our case `robots`)\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace/?WT.mc_id=absa-notebook-abornst) object from the existing workspace you created in the Prerequisites step or create a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hal\twestus2\trobots\twestus2\n",
      "Library configuration succeeded\n"
     ]
    }
   ],
   "source": [
    "#subscription_id = ''\n",
    "#resource_group  = 'absa'\n",
    "#workspace_name  = 'absa_space'\n",
    "#ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "#ws.write_config()\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two computer option run once(preview) and persistent compute for this demo we will use persistent compute to learn more about run once compute check out the [docs](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute?WT.mc_id=absa-notebook-abornst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for your CPU cluster\n",
    "cluster_name = \"gandalf\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           vm_priority='lowpriority',\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=4)\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Datastore Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datastore.get(ws, 'absa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning NLP Archictect  with AzureML HyperDrive\n",
    "Although ABSA is an unsupervised method it's hyper parameters such as the aspect and opinion word thresholds can be fined tuned if provided with a small sample of labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sampling = RandomParameterSampling({\n",
    "         '--asp_thresh': choice(range(2,5)),\n",
    "         '--op_thresh': choice(range(2,5)), \n",
    "         '--max_iter': choice(range(2,5))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Termination Policy\n",
    "First we will define an early terminination policy. [Median stopping](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.medianstoppingpolicy?WT.mc_id=absa-notebook-abornst) is an early termination policy based on running averages of primary metrics reported by the runs. This policy computes running averages across all training runs and terminates runs whose performance is worse than the median of the running averages. \n",
    "\n",
    "This policy takes the following configuration parameters:\n",
    "\n",
    "- evaluation_interval: the frequency for applying the policy (optional parameter).\n",
    "- delay_evaluation: delays the first policy evaluation for a specified number of intervals (optional parameter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#specify-early-termination-policy?WT.mc_id=absa-notebook-abornst) for more information on the Median stopping policy and other policies available.\n",
    "\n",
    "Early termination policy is effective when your training script reports primary metric periodically, several times during the execution. In this case, when the metric is not getting better, the script can be terminated. In our case, since we report the metric only at the end of the execution, early termination is not effective.\n",
    "\n",
    "Finally, we define our Hyper Drive configuration to maximize our Model's weighted F1 score. Hyper Drive can optimize any metric can be optimized as long as it's logged by the training script. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_est = Estimator(source_directory='.',\n",
    "                   compute_target=cluster,\n",
    "                   environment_variables = {'NLP_ARCHITECT_BE':'CPU'},\n",
    "                   entry_script='train.py',\n",
    "                   pip_packages=['git+https://github.com/NervanaSystems/nlp-architect.git@absa',\n",
    "                                 'spacy==2.1.8']\n",
    ")\n",
    "\n",
    "hd_config = HyperDriveConfig(estimator=nlp_est,\n",
    "                            hyperparameter_sampling=param_sampling,\n",
    "                            #policy=early_termination_policy,\n",
    "                            primary_metric_name='accuracy',\n",
    "                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                            max_total_runs=16,\n",
    "                            max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name='absa_hyperdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = experiment.submit(hd_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absa_hyperdrive_1579824130849136'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperdrive_run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = [r for r in experiment.get_runs() if r.id == 'absa_hyperdrive_1579824130849136'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor HyperDrive runs\n",
    "We can monitor the progress of the runs with the following Jupyter widget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c263fd67c65b408da32125bc4d342724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_HyperDriveWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Canceled\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/absa_hyperdrive/runs/absa_hyperdrive_1579824130849136?wsid=/subscriptions/91d27443-f037-45d9-bb0c-428256992df6/resourcegroups/robots/workspaces/hal\", \"run_id\": \"absa_hyperdrive_1579824130849136\", \"run_properties\": {\"run_id\": \"absa_hyperdrive_1579824130849136\", \"created_utc\": \"2020-01-24T00:02:10.960596Z\", \"properties\": {\"primary_metric_config\": \"{\\\"name\\\": \\\"accuracy\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"resume_from\": \"null\", \"runTemplate\": \"HyperDrive\", \"azureml.runsource\": \"hyperdrive\", \"platform\": \"AML\", \"ContentSnapshotId\": \"55b51c87-f43b-4245-a581-87ba6e935750\"}, \"tags\": {\"max_concurrent_jobs\": \"4\", \"max_total_jobs\": \"16\", \"max_duration_minutes\": \"10080\", \"policy_config\": \"{\\\"name\\\": \\\"DEFAULT\\\"}\", \"generator_config\": \"{\\\"name\\\": \\\"RANDOM\\\", \\\"parameter_space\\\": {\\\"--asp_thresh\\\": [\\\"choice\\\", [[2, 3, 4]]], \\\"--op_thresh\\\": [\\\"choice\\\", [[2, 3, 4]]], \\\"--max_iter\\\": [\\\"choice\\\", [[2, 3, 4]]]}}\", \"primary_metric_config\": \"{\\\"name\\\": \\\"accuracy\\\", \\\"goal\\\": \\\"maximize\\\"}\", \"platform_config\": \"{\\\"ServiceAddress\\\": \\\"https://westus2.experiments.azureml.net\\\", \\\"ServiceArmScope\\\": \\\"subscriptions/91d27443-f037-45d9-bb0c-428256992df6/resourceGroups/robots/providers/Microsoft.MachineLearningServices/workspaces/hal/experiments/absa_hyperdrive\\\", \\\"SubscriptionId\\\": \\\"91d27443-f037-45d9-bb0c-428256992df6\\\", \\\"ResourceGroupName\\\": \\\"robots\\\", \\\"WorkspaceName\\\": \\\"hal\\\", \\\"ExperimentName\\\": \\\"absa_hyperdrive\\\", \\\"Definition\\\": {\\\"Overrides\\\": {\\\"script\\\": \\\"train.py\\\", \\\"arguments\\\": [], \\\"target\\\": \\\"gandalf\\\", \\\"framework\\\": \\\"Python\\\", \\\"communicator\\\": \\\"None\\\", \\\"maxRunDurationSeconds\\\": null, \\\"nodeCount\\\": 1, \\\"environment\\\": {\\\"name\\\": null, \\\"version\\\": null, \\\"environmentVariables\\\": {\\\"NLP_ARCHITECT_BE\\\": \\\"CPU\\\"}, \\\"python\\\": {\\\"userManagedDependencies\\\": false, \\\"interpreterPath\\\": \\\"python\\\", \\\"condaDependenciesFile\\\": null, \\\"baseCondaEnvironment\\\": null, \\\"condaDependencies\\\": {\\\"name\\\": \\\"project_environment\\\", \\\"dependencies\\\": [\\\"python=3.6.2\\\", {\\\"pip\\\": [\\\"azureml-defaults\\\", \\\"git+https://github.com/NervanaSystems/nlp-architect.git@absa\\\", \\\"spacy==2.1.8\\\"]}], \\\"channels\\\": [\\\"conda-forge\\\"]}}, \\\"docker\\\": {\\\"enabled\\\": true, \\\"baseImage\\\": \\\"mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04\\\", \\\"baseDockerfile\\\": null, \\\"sharedVolumes\\\": true, \\\"shmSize\\\": \\\"2g\\\", \\\"arguments\\\": [], \\\"baseImageRegistry\\\": {\\\"address\\\": null, \\\"username\\\": null, \\\"password\\\": null}}, \\\"spark\\\": {\\\"repositories\\\": [], \\\"packages\\\": [], \\\"precachePackages\\\": false}, \\\"databricks\\\": {\\\"mavenLibraries\\\": [], \\\"pypiLibraries\\\": [], \\\"rcranLibraries\\\": [], \\\"jarLibraries\\\": [], \\\"eggLibraries\\\": []}, \\\"inferencingStackVersion\\\": null}, \\\"history\\\": {\\\"outputCollection\\\": true, \\\"snapshotProject\\\": true, \\\"directoriesToWatch\\\": [\\\"logs\\\"]}, \\\"spark\\\": {\\\"configuration\\\": {\\\"spark.app.name\\\": \\\"Azure ML Experiment\\\", \\\"spark.yarn.maxAppAttempts\\\": 1}}, \\\"hdi\\\": {\\\"yarnDeployMode\\\": \\\"cluster\\\"}, \\\"tensorflow\\\": {\\\"workerCount\\\": 1, \\\"parameterServerCount\\\": 1}, \\\"mpi\\\": {\\\"processCountPerNode\\\": 1}, \\\"dataReferences\\\": {}, \\\"data\\\": {}, \\\"sourceDirectoryDataStore\\\": null, \\\"amlcompute\\\": {\\\"vmSize\\\": null, \\\"vmPriority\\\": null, \\\"retainCluster\\\": false, \\\"name\\\": null, \\\"clusterMaxNodeCount\\\": 1}}, \\\"TargetDetails\\\": null, \\\"SnapshotId\\\": \\\"55b51c87-f43b-4245-a581-87ba6e935750\\\", \\\"TelemetryValues\\\": {\\\"amlClientType\\\": \\\"azureml-sdk-train\\\", \\\"amlClientModule\\\": \\\"azureml.train.hyperdrive._search\\\", \\\"amlClientFunction\\\": \\\"search\\\", \\\"tenantId\\\": \\\"72f988bf-86f1-41af-91ab-2d7cd011db47\\\", \\\"amlClientRequestId\\\": \\\"89c68b20-7a7a-4f11-9757-9f69e7ebc246\\\", \\\"amlClientSessionId\\\": \\\"9d85aa5e-69b4-4ed4-8974-4aebe7eb8294\\\", \\\"subscriptionId\\\": \\\"91d27443-f037-45d9-bb0c-428256992df6\\\", \\\"estimator\\\": \\\"Estimator\\\", \\\"samplingMethod\\\": \\\"RANDOM\\\", \\\"terminationPolicy\\\": \\\"Default\\\", \\\"primaryMetricGoal\\\": \\\"maximize\\\", \\\"maxTotalRuns\\\": 16, \\\"maxConcurrentRuns\\\": 4, \\\"maxDurationMinutes\\\": 10080, \\\"computeTarget\\\": \\\"AmlCompute\\\", \\\"vmSize\\\": null}}}\", \"resume_child_runs\": \"null\", \"all_jobs_generated\": \"true\", \"cancellation_requested\": \"true\", \"progress_metadata_evaluation_timestamp\": \"\\\"2020-01-24T00:02:12.545979\\\"\", \"progress_metadata_digest\": \"\\\"1ac3f4af91fe2bd8b24f8c88022b78069e1fc67aae8be3880b517c74eec6353d\\\"\", \"progress_metadata_active_timestamp\": \"\\\"2020-01-24T00:02:12.545979\\\"\", \"absa_hyperdrive_1579824130849136_0\": \"{\\\"--asp_thresh\\\": 4, \\\"--max_iter\\\": 4, \\\"--op_thresh\\\": 3}\", \"absa_hyperdrive_1579824130849136_1\": \"{\\\"--asp_thresh\\\": 2, \\\"--max_iter\\\": 2, \\\"--op_thresh\\\": 3}\", \"absa_hyperdrive_1579824130849136_2\": \"{\\\"--asp_thresh\\\": 4, \\\"--max_iter\\\": 3, \\\"--op_thresh\\\": 4}\", \"absa_hyperdrive_1579824130849136_3\": \"{\\\"--asp_thresh\\\": 3, \\\"--max_iter\\\": 3, \\\"--op_thresh\\\": 3}\", \"environment_preparation_status\": \"PREPARED\", \"prepare_run_id\": \"absa_hyperdrive_1579824130849136_preparation\", \"absa_hyperdrive_1579824130849136_4\": \"{\\\"--asp_thresh\\\": 3, \\\"--max_iter\\\": 4, \\\"--op_thresh\\\": 3}\", \"absa_hyperdrive_1579824130849136_5\": \"{\\\"--asp_thresh\\\": 3, \\\"--max_iter\\\": 4, \\\"--op_thresh\\\": 2}\", \"absa_hyperdrive_1579824130849136_6\": \"{\\\"--asp_thresh\\\": 2, \\\"--max_iter\\\": 2, \\\"--op_thresh\\\": 4}\", \"absa_hyperdrive_1579824130849136_7\": \"{\\\"--asp_thresh\\\": 2, \\\"--max_iter\\\": 2, \\\"--op_thresh\\\": 2}\", \"absa_hyperdrive_1579824130849136_4_cancelled\": \"true\", \"absa_hyperdrive_1579824130849136_5_cancelled\": \"true\", \"absa_hyperdrive_1579824130849136_6_cancelled\": \"true\", \"absa_hyperdrive_1579824130849136_7_cancelled\": \"true\"}, \"end_time_utc\": \"2020-01-24T00:13:46.928392Z\", \"status\": \"Canceled\", \"log_files\": {\"azureml-logs/hyperdrive.txt\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_hyperdrive_1579824130849136/azureml-logs/hyperdrive.txt?sv=2019-02-02&sr=b&sig=e8aA7D8Dc8N%2FT%2BO9oG83TNFLOzmVJtd%2Fk1QNyssNDcY%3D&st=2020-01-24T00%3A04%3A05Z&se=2020-01-24T08%3A14%3A05Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/hyperdrive.txt\"]], \"run_duration\": \"0:11:35\", \"hyper_parameters\": {\"--asp_thresh\": [\"choice\", [[2, 3, 4]]], \"--op_thresh\": [\"choice\", [[2, 3, 4]]], \"--max_iter\": [\"choice\", [[2, 3, 4]]]}}, \"child_runs\": [{\"run_id\": \"absa_hyperdrive_1579824130849136_3\", \"run_number\": 33, \"metric\": null, \"status\": \"Failed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-01-24T00:09:31.105458Z\", \"end_time\": \"2020-01-24T00:11:07.218016Z\", \"created_time\": \"2020-01-24T00:05:46.762053Z\", \"created_time_dt\": \"2020-01-24T00:05:46.762053Z\", \"duration\": \"0:05:20\", \"hyperdrive_id\": \"1579824130849136\", \"arguments\": null, \"param_--asp_thresh\": 3, \"param_--max_iter\": 3, \"param_--op_thresh\": 3}, {\"run_id\": \"absa_hyperdrive_1579824130849136_2\", \"run_number\": 32, \"metric\": null, \"status\": \"Failed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-01-24T00:07:53.483248Z\", \"end_time\": \"2020-01-24T00:09:26.896243Z\", \"created_time\": \"2020-01-24T00:05:45.816331Z\", \"created_time_dt\": \"2020-01-24T00:05:45.816331Z\", \"duration\": \"0:03:41\", \"hyperdrive_id\": \"1579824130849136\", \"arguments\": null, \"param_--asp_thresh\": 4, \"param_--max_iter\": 3, \"param_--op_thresh\": 4}, {\"run_id\": \"absa_hyperdrive_1579824130849136_1\", \"run_number\": 31, \"metric\": null, \"status\": \"Failed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-01-24T00:06:16.812152Z\", \"end_time\": \"2020-01-24T00:07:49.79014Z\", \"created_time\": \"2020-01-24T00:05:45.689837Z\", \"created_time_dt\": \"2020-01-24T00:05:45.689837Z\", \"duration\": \"0:02:04\", \"hyperdrive_id\": \"1579824130849136\", \"arguments\": null, \"param_--asp_thresh\": 2, \"param_--max_iter\": 2, \"param_--op_thresh\": 3}, {\"run_id\": \"absa_hyperdrive_1579824130849136_0\", \"run_number\": 30, \"metric\": null, \"status\": \"Failed\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-01-24T00:11:04.845393Z\", \"end_time\": \"2020-01-24T00:12:50.555088Z\", \"created_time\": \"2020-01-24T00:05:45.641741Z\", \"created_time_dt\": \"2020-01-24T00:05:45.641741Z\", \"duration\": \"0:07:04\", \"hyperdrive_id\": \"1579824130849136\", \"arguments\": null, \"param_--asp_thresh\": 4, \"param_--max_iter\": 4, \"param_--op_thresh\": 3}, {\"run_id\": \"absa_hyperdrive_1579824130849136_4\", \"run_number\": 34, \"metric\": null, \"status\": \"Canceled\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-01-24T00:12:48.764832Z\", \"end_time\": \"2020-01-24T00:13:31.83472Z\", \"created_time\": \"2020-01-24T00:08:19.65703Z\", \"created_time_dt\": \"2020-01-24T00:08:19.65703Z\", \"duration\": \"0:05:12\", \"hyperdrive_id\": \"1579824130849136\", \"arguments\": null, \"param_--asp_thresh\": 3, \"param_--max_iter\": 4, \"param_--op_thresh\": 3}, {\"run_id\": \"absa_hyperdrive_1579824130849136_5\", \"run_number\": 35, \"metric\": null, \"status\": \"Canceled\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-01-24T00:09:51.720841Z\", \"end_time\": \"2020-01-24T00:13:34.771588Z\", \"created_time\": \"2020-01-24T00:09:51.720841Z\", \"created_time_dt\": \"2020-01-24T00:09:51.720841Z\", \"duration\": \"0:03:43\", \"hyperdrive_id\": \"1579824130849136\", \"arguments\": null, \"param_--asp_thresh\": 3, \"param_--max_iter\": 4, \"param_--op_thresh\": 2}, {\"run_id\": \"absa_hyperdrive_1579824130849136_6\", \"run_number\": 36, \"metric\": null, \"status\": \"Canceled\", \"run_type\": \"azureml.scriptrun\", \"training_percent\": null, \"start_time\": \"2020-01-24T00:11:24.081499Z\", \"end_time\": \"2020-01-24T00:13:33.035592Z\", \"created_time\": \"2020-01-24T00:11:24.081499Z\", \"created_time_dt\": \"2020-01-24T00:11:24.081499Z\", \"duration\": \"0:02:08\", \"hyperdrive_id\": \"1579824130849136\", \"arguments\": null, \"param_--asp_thresh\": 2, \"param_--max_iter\": 2, \"param_--op_thresh\": 4}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2020-01-24T00:02:12.813596][GENERATOR][INFO]Trying to sample '4' jobs from the hyperparameter space\\r\\n[2020-01-24T00:02:13.166678][GENERATOR][INFO]Successfully sampled '4' jobs, they will soon be submitted to the execution target.\\r\\n[2020-01-24T00:02:11.170118][API][INFO]Experiment created\\r\\n[2020-01-24T00:05:13.8821103Z][SCHEDULER][INFO]The execution environment is being prepared. Please be patient as it can take a few minutes.\\r\\n[2020-01-24T00:05:45.1621393Z][SCHEDULER][INFO]Scheduling job, id='absa_hyperdrive_1579824130849136_1'\\r\\n[2020-01-24T00:05:45.1630705Z][SCHEDULER][INFO]Scheduling job, id='absa_hyperdrive_1579824130849136_2'\\r\\n[2020-01-24T00:05:45.1640031Z][SCHEDULER][INFO]Scheduling job, id='absa_hyperdrive_1579824130849136_3'\\r\\n[2020-01-24T00:05:45.1610397Z][SCHEDULER][INFO]Scheduling job, id='absa_hyperdrive_1579824130849136_0'\\r\\n[2020-01-24T00:05:45.1605276Z][SCHEDULER][INFO]The execution environment was successfully prepared.\\r\\n[2020-01-24T00:05:45.7606085Z][SCHEDULER][INFO]Successfully scheduled a job. Id='absa_hyperdrive_1579824130849136_1'\\r\\n[2020-01-24T00:05:45.8803582Z][SCHEDULER][INFO]Successfully scheduled a job. Id='absa_hyperdrive_1579824130849136_2'\\r\\n[2020-01-24T00:05:45.8392758Z][SCHEDULER][INFO]Successfully scheduled a job. Id='absa_hyperdrive_1579824130849136_0'\\r\\n[2020-01-24T00:05:46.8318811Z][SCHEDULER][INFO]Successfully scheduled a job. Id='absa_hyperdrive_1579824130849136_3'\\r\\n[2020-01-24T00:08:13.411227][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\\r\\n[2020-01-24T00:08:13.573943][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\\r\\n[2020-01-24T00:08:18.8933947Z][SCHEDULER][INFO]Scheduling job, id='absa_hyperdrive_1579824130849136_4'\\r\\n[2020-01-24T00:08:19.7195274Z][SCHEDULER][INFO]Successfully scheduled a job. Id='absa_hyperdrive_1579824130849136_4'\\r\\n[2020-01-24T00:09:43.754008][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\\r\\n[2020-01-24T00:09:43.926053][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\\r\\n[2020-01-24T00:09:51.2375608Z][SCHEDULER][INFO]Scheduling job, id='absa_hyperdrive_1579824130849136_5'\\r\\n[2020-01-24T00:09:51.9647654Z][SCHEDULER][INFO]Successfully scheduled a job. Id='absa_hyperdrive_1579824130849136_5'\\r\\n[2020-01-24T00:11:13.625295][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\\r\\n[2020-01-24T00:11:13.754022][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\\r\\n[2020-01-24T00:11:23.5067484Z][SCHEDULER][INFO]Scheduling job, id='absa_hyperdrive_1579824130849136_6'\\r\\n[2020-01-24T00:11:24.1353612Z][SCHEDULER][INFO]Successfully scheduled a job. Id='absa_hyperdrive_1579824130849136_6'\\r\\n[2020-01-24T00:13:13.504667][GENERATOR][INFO]Trying to sample '1' jobs from the hyperparameter space\\r\\n[2020-01-24T00:13:13.648070][GENERATOR][INFO]Successfully sampled '1' jobs, they will soon be submitted to the execution target.\\r\\n[2020-01-24T00:13:17.110160][CONTROLLER][INFO]Experiment has been marked for cancellation.\\r\\n[2020-01-24T00:13:17.110327][CONTROLLER][WARNING]The first 3 jobs have failed. The system is canceling the experiment. Please resolve the issues before resubmitting the experiment.\\r\\n[2020-01-24T00:13:25.6510815Z][SCHEDULER][INFO]Cancelling job, id='absa_hyperdrive_1579824130849136_4'\\r\\n[2020-01-24T00:13:25.6526440Z][SCHEDULER][INFO]Cancelling job, id='absa_hyperdrive_1579824130849136_6'\\r\\n[2020-01-24T00:13:25.6519548Z][SCHEDULER][INFO]Cancelling job, id='absa_hyperdrive_1579824130849136_5'\\r\\n[2020-01-24T00:13:26.1903567Z][SCHEDULER][INFO]Updating job statuses to cancelled: [(job id = 'absa_hyperdrive_1579824130849136_4', previous status = 'RUNNING'), (job id = 'absa_hyperdrive_1579824130849136_5', previous status = 'SCHEDULED'), (job id = 'absa_hyperdrive_1579824130849136_6', previous status = 'SCHEDULED'), (job id = 'absa_hyperdrive_1579824130849136_7', previous status = 'QUEUED')]\\r\\n[2020-01-24T00:13:47.210983][CONTROLLER][INFO]Experiment was 'ExperimentStatus.RUNNING', is 'ExperimentStatus.CANCELLED'.\\n\\nRun is canceled.\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the best model\n",
    "Once all the runs complete, we can find the run that produced the model with the highest evaluation (METRIC TBD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run)\n",
    "print('Best Run is:\\n  F1: {0:.5f}'.format(\n",
    "        best_run_metrics['f1_weighted']\n",
    "     ))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
