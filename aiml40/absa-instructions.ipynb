{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License.\n",
    "\n",
    "### Intel NLP-Architect ABSA on AzureML \n",
    "\n",
    "### INSTRUCTOR VERSION\n",
    "\n",
    "> **This instructor version of the notebook gives additional instructions as to which cells should be run in demo mode, and which should not. It assumes that before the demo you will execute the complete notebook, and then during the demo certain cells would be re-run to demonstrate working process.**\n",
    "\n",
    "This notebook contains an end-to-end walkthrough of using Azure Machine Learning Service to train, finetune and test [Aspect Based Sentiment Analysis Models using Intel's NLP Architect](http://nlp_architect.nervanasys.com/absa.html)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* Understand the architecture and terms introduced by Azure Machine Learning (AML)\n",
    "* Have working Jupyter Notebook Environment. You can:\n",
    "    - Install Python environment locally, as described below in **Local Installation**\n",
    "    - Use [Azure Notebooks](https://docs.microsoft.com/ru-ru/azure/notebooks/azure-notebooks-overview/?wt.mc_id=absa-notebook-abornst). In this case you should upload the `absa.ipynb` file to a new Azure Notebooks project, or just clone the [GitHub Repo](https://github.com/microsoft/ignite-learning-paths-training-aiml/tree/master/aiml40).\n",
    "* Azure Machine Learning Workspace in your Azure Subscription\n",
    "\n",
    "#### Local Installation\n",
    "\n",
    "Install the Python SDK: make sure to install notebook, and contrib:\n",
    "\n",
    "```shell\n",
    "conda create -n azureml -y Python=3.6\n",
    "source activate azureml\n",
    "pip install --upgrade azureml-sdk[notebooks,contrib] \n",
    "conda install ipywidgets\n",
    "jupyter nbextension install --py --user azureml.widgets\n",
    "jupyter nbextension enable azureml.widgets --user --py\n",
    "```\n",
    "\n",
    "You will need to restart jupyter after this Detailed instructions are [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-create-workspace-with-python/?WT.mc_id=absa-notebook-abornst)\n",
    "\n",
    "If you need a free trial account to get started you can get one [here](https://azure.microsoft.com/en-us/offers/ms-azr-0044p/?WT.mc_id=absa-notebook-abornst)\n",
    "\n",
    "#### Creating Azure ML Workspace\n",
    "\n",
    "Azure ML Workspace can be created by using one of the following ways:\n",
    "* Manually through [Azure Portal](http://portal.azure.com/?WT.mc_id=absa-notebook-abornst) - [here is the complete walkthrough](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-manage-workspace/?wt.mc_id=absa-notebook-abornst)\n",
    "* Using [Azure CLI](https://docs.microsoft.com/ru-ru/cli/azure/?view=azure-cli-latest&wt.mc_id=absa-notebook-abornst), using the following commands:\n",
    "\n",
    "```shell\n",
    "az extension add -n azure-cli-ml\n",
    "az group create -n absa -l westus2\n",
    "az ml workspace create -w absa_space -g absa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "To access an Azure ML Workspace, you will need to import the AML library and the following information:\n",
    "* A name for your workspace (in our example - `absa_space`)\n",
    "* Your subscription id (can be obtained by running `az account list`)\n",
    "* The resource group name (in our case `absa`)\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace/?WT.mc_id=absa-notebook-abornst) object from the existing workspace you created in the Prerequisites step or create a new one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **This cell can be run without problem, because it will just create a connection object for the workspace. Make sure to insert the correct `subscription_id` value before use, or have `config.json` file ready.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hal\twestus2\trobots\twestus2\n",
      "Library configuration succeeded\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "#subscription_id = ''\n",
    "#resource_group  = 'absa'\n",
    "#workspace_name  = 'absa_space'\n",
    "#ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "#ws.write_config()\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two computer option run once(preview) and persistent compute for this demo we will use persistent compute to learn more about run once compute check out the [docs](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute?WT.mc_id=absa-notebook-abornst)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **This cell can be run because it will not re-create a cluster. Although it does not make much sense to run it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n",
      "Succeeded\n",
      "AmlCompute wait for completion finished\n",
      "Minimum number of nodes requested have been provisioned\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cluster_name = \"gandalf\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           vm_priority='lowpriority',\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=4)\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data\n",
    "\n",
    "The dataset we are using comes from the [womens ecommerce clothing reviews dataset](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/) and is in the open domain, this can be replaced with any csv file with rows of text as the absa model is unsupervised. \n",
    "\n",
    "The documentation for uploading data can be found [here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.azure_storage_datastore.azureblobdatastore/?WT.mc_id=absa-notebook-abornst) for now we will us the ds.upload command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using as a separate notebook - fetch files from github repo\n",
    "if not os.path.isdir('dataset'):\n",
    "    !mkdir dataset\n",
    "    !wget -O 'dataset/clothing_absa_train_small.csv' 'https://raw.githubusercontent.com/microsoft/ignite-learning-paths-training-aiml/master/aiml40/dataset/clothing_absa_train_small.csv'\n",
    "    !wget -O 'dataset/clothing_absa_train.csv' 'https://raw.githubusercontent.com/microsoft/ignite-learning-paths-training-aiml/master/aiml40/dataset/clothing_absa_train.csv'\n",
    "    !wget -O 'dataset/clothing-absa-validation.json' 'https://raw.githubusercontent.com/microsoft/ignite-learning-paths-training-aiml/master/aiml40/dataset/clothing-absa-validation.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O 'dataset/glove.840B.300d.zip' 'http://nlp.stanford.edu/data/glove.840B.300d.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os                            \n",
    "#lib_root = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "#ds = ws.get_default_datastore()\n",
    "#ds.upload('./dataset', target_path='clothing_data', overwrite=True, show_progress=True)\n",
    "from azureml.core import Datastore\n",
    "ds = Datastore.get(ws, 'absa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the the glove file is uploaded to our datastore we can remove it from our local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm 'dataset/glove.840B.300d.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Expierment\n",
    "\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment/?WT.mc_id=absa-notebook-abornst) to track all the runs in your workspace for this distributed PyTorch tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **In most of the cases, you want to skip the following 3 cells during the demo, in order not to run the experiment again. However, you may also start another experiment if time permists, in which case you can run them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'absa'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--data_folder': ds,\n",
    "}\n",
    "\n",
    "nlp_est = Estimator(source_directory='.',\n",
    "                   script_params=script_params,\n",
    "                   compute_target=cluster,\n",
    "                   environment_variables = {'NLP_ARCHITECT_BE':'CPU'},\n",
    "                   entry_script='train.py',\n",
    "                   pip_packages=['git+https://github.com/NervanaSystems/nlp-architect.git@absa',\n",
    "                                 'spacy==2.1.8']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(nlp_est)\n",
    "run_id = run.id\n",
    "print(run_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you accidently run the following cell more than once you can cancel a run with the run.cancel() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **To retrieve the run, we use run id here. It can either be hard-coded from the previous pre-demo run, or you can rely on the jupyter kernel not restarting, in which case it will be saved in the `run_id` variable. So, if the jupyter engine has not been restarted, you may run cell 2, otherwise run cell 1** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1dcb33aafc89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_runs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'absa_1568985331_df076c3c'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'exp' is not defined"
     ]
    }
   ],
   "source": [
    "run = [r for r in exp.get_runs() if r.id == 'absa_1568985331_df076c3c'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = [r for r in exp.get_runs() if r.id == run_id][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Run this to show the result of the run, either in progress or completed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning NLP Archictect  with AzureML HyperDrive\n",
    "Although ABSA is an unsupervised method it's hyper parameters such as the aspect and opinion word thresholds can be fined tuned if provided with a small sample of labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import *\n",
    "import math\n",
    "\n",
    "param_sampling = RandomParameterSampling({\n",
    "         '--asp_thresh': choice(range(2,5)),\n",
    "         '--op_thresh': choice(range(2,5)), \n",
    "         '--max_iter': choice(range(2,5))\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Termination Policy\n",
    "First we will define an early terminination policy. [Median stopping](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.medianstoppingpolicy?WT.mc_id=absa-notebook-abornst) is an early termination policy based on running averages of primary metrics reported by the runs. This policy computes running averages across all training runs and terminates runs whose performance is worse than the median of the running averages. \n",
    "\n",
    "This policy takes the following configuration parameters:\n",
    "\n",
    "- evaluation_interval: the frequency for applying the policy (optional parameter).\n",
    "- delay_evaluation: delays the first policy evaluation for a specified number of intervals (optional parameter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters#specify-early-termination-policy?WT.mc_id=absa-notebook-abornst) for more information on the Median stopping policy and other policies available.\n",
    "\n",
    "Now that we've defined our early termination policy we can define our Hyper Drive configuration to maximize our Model's weighted F1 score. Hyper Drive can optimize any metric can be optimized as long as it's logged by the training script. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_config = HyperDriveConfig(estimator=nlp_est,\n",
    "                            hyperparameter_sampling=param_sampling,\n",
    "                            policy=early_termination_policy,\n",
    "                            primary_metric_name='f1_weighted',\n",
    "                            primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                            max_total_runs=16,\n",
    "                            max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lauch the hyperparameter tuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(workspace=ws, name='absa_hyperdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = experiment.submit(hd_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run = [r for r in experiment.get_runs() if r.id == 'absa_hyperdrive_1578973612991526'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor HyperDrive runs\n",
    "We can monitor the progress of the runs with the following Jupyter widget. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find and register the best model\n",
    "Once all the runs complete, we can find the run that produced the model with the highest evaluation (METRIC TBD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: absa_hyperdrive,\n",
      "Id: absa_hyperdrive_1578973612991526_1,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Completed)\n",
      "Best Run is:\n",
      "  F1: 0.89455\n"
     ]
    }
   ],
   "source": [
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run)\n",
    "print('Best Run is:\\n  F1: {0:.5f}'.format(\n",
    "        best_run_metrics['f1_weighted']\n",
    "     ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['azureml-logs/55_azureml-execution-tvmps_f0be58f37b4253cd11263408e3571dc29440bc3a9e32c89331b3a8ff6ea8b0d5_d.txt',\n",
       " 'azureml-logs/65_job_prep-tvmps_f0be58f37b4253cd11263408e3571dc29440bc3a9e32c89331b3a8ff6ea8b0d5_d.txt',\n",
       " 'azureml-logs/70_driver_log.txt',\n",
       " 'azureml-logs/75_job_post-tvmps_f0be58f37b4253cd11263408e3571dc29440bc3a9e32c89331b3a8ff6ea8b0d5_d.txt',\n",
       " 'azureml-logs/process_info.json',\n",
       " 'azureml-logs/process_status.json',\n",
       " 'logs/azureml/153_azureml.log',\n",
       " 'logs/azureml/job_prep_azureml.log',\n",
       " 'logs/azureml/job_release_azureml.log',\n",
       " 'outputs/generated_aspect_lex.csv',\n",
       " 'outputs/generated_opinion_lex_reranked.csv',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.0.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.1.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.10.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.100.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.101.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.102.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.103.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.104.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.105.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.106.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.107.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.108.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.109.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.11.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.110.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.111.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.112.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.113.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.114.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.115.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.116.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.117.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.118.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.119.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.12.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.120.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.121.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.122.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.123.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.124.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.125.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.126.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.127.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.128.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.129.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.13.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.130.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.131.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.132.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.133.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.134.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.135.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.136.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.137.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.138.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.139.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.14.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.140.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.141.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.142.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.143.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.144.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.145.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.146.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.147.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.148.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.149.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.15.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.150.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.151.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.152.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.153.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.154.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.155.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.156.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.157.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.158.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.159.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.16.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.160.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.161.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.162.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.163.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.164.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.165.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.166.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.167.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.168.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.169.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.17.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.170.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.171.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.172.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.173.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.174.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.175.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.176.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.177.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.178.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.179.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.18.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.180.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.181.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.182.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.183.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.184.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.185.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.186.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.187.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.188.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.189.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.19.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.190.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.191.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.192.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.193.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.194.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.195.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.196.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.197.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.198.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.199.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.2.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.20.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.200.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.201.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.202.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.203.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.204.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.205.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.206.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.207.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.208.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.209.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.21.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.210.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.211.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.212.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.213.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.214.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.215.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.216.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.217.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.218.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.219.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.22.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.220.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.221.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.222.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.223.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.224.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.225.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.226.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.227.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.228.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.229.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.23.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.230.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.231.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.232.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.233.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.234.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.235.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.236.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.237.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.238.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.239.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.24.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.240.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.241.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.242.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.243.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.244.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.245.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.246.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.247.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.248.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.249.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.25.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.250.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.251.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.252.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.253.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.254.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.255.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.256.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.257.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.258.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.259.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.26.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.260.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.261.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.262.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.263.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.264.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.265.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.266.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.267.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.268.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.269.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.27.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.270.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.271.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.272.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.273.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.274.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.275.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.276.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.277.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.278.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.279.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.28.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.280.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.281.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.282.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.283.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.284.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.285.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.286.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.287.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.288.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.289.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.29.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.290.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.291.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.292.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.293.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.294.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.295.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.296.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.297.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.298.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.299.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.3.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.30.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.300.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.301.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.302.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.303.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.304.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.305.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.306.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.307.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.308.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.309.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.31.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.310.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.311.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.312.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.313.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.314.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.315.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.316.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.317.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.318.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.319.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.32.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.320.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.321.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.322.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.323.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.324.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.325.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.326.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.327.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.328.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.329.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.33.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.330.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.331.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.332.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.333.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.334.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.335.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.336.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.337.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.338.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.339.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.34.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.340.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.341.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.342.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.343.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.344.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.345.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.346.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.347.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.348.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.349.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.35.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.350.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.351.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.352.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.353.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.354.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.355.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.356.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.357.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.358.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.359.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.36.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.360.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.361.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.362.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.363.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.364.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.365.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.366.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.367.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.368.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.369.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.37.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.370.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.371.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.372.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.373.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.374.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.375.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.376.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.377.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.378.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.379.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.38.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.380.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.381.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.382.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.383.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.384.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.385.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.386.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.387.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.388.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.389.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.39.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.390.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.391.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.392.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.393.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.394.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.395.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.396.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.397.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.398.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.399.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.4.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.40.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.400.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.401.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.402.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.403.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.404.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.405.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.406.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.407.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.408.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.409.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.41.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.410.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.411.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.412.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.413.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.414.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.415.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.416.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.417.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.418.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.419.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.42.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.420.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.421.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.422.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.423.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.424.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.425.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.426.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.427.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.428.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.429.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.43.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.430.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.431.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.432.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.433.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.434.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.435.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.436.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.437.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.438.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.439.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.44.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.440.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.441.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.442.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.443.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.444.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.445.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.446.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.447.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.448.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.449.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.45.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.450.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.451.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.452.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.453.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.454.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.455.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.456.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.457.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.458.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.459.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.46.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.460.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.461.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.462.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.463.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.464.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.465.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.466.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.467.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.468.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.469.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.47.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.470.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.471.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.472.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.473.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.474.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.475.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.476.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.477.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.478.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.479.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.48.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.480.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.481.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.482.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.483.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.484.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.485.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.486.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.487.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.488.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.489.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.49.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.490.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.491.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.492.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.493.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.494.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.495.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.496.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.497.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.498.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.499.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.5.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.50.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.500.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.501.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.502.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.503.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.504.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.505.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.506.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.507.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.508.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.509.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.51.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.510.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.511.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.512.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.513.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.514.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.515.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.516.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.517.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.518.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.519.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.52.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.520.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.521.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.522.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.523.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.524.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.525.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.526.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.527.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.528.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.529.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.53.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.530.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.531.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.532.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.533.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.534.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.535.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.536.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.537.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.538.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.539.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.54.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.540.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.541.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.542.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.543.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.544.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.545.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.546.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.547.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.548.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.549.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.55.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.550.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.551.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.552.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.553.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.554.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.555.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.556.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.557.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.558.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.559.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.56.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.560.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.561.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.562.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.563.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.564.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.565.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.566.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.567.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.568.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.569.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.57.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.570.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.571.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.572.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.573.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.574.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.575.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.576.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.577.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.58.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.59.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.6.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.60.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.61.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.62.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.63.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.64.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.65.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.66.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.67.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.68.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.69.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.7.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.70.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.71.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.72.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.73.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.74.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.75.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.76.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.77.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.78.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.79.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.8.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.80.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.81.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.82.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.83.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.84.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.85.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.86.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.87.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.88.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.89.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.9.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.90.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.91.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.92.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.93.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.94.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.95.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.96.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.97.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.98.json',\n",
       " 'outputs/parsed/clothing_absa_train_small/0.99.json']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run.download_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile, rmtree\n",
    "if os.path.exists('model'):\n",
    "    rmtree('model')\n",
    "    \n",
    "os.makedirs('model')\n",
    "\n",
    "aspect_lex = copyfile('outputs/generated_aspect_lex.csv', 'model/generated_aspect_lex.csv')\n",
    "opinion_lex = copyfile('outputs/generated_opinion_lex_reranked.csv', 'model/generated_opinion_lex_reranked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AzureMLException",
     "evalue": "AzureMLException:\n\tMessage: UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.absa_hyperdrive_1578973612991526_1/model/generated_aspect_lex.csv already exists.\nUserError: Resource Conflict: ArtifactId ExperimentRun/dcid.absa_hyperdrive_1578973612991526_1/model/generated_opinion_lex_reranked.csv already exists.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.absa_hyperdrive_1578973612991526_1/model/generated_aspect_lex.csv already exists.\\nUserError: Resource Conflict: ArtifactId ExperimentRun/dcid.absa_hyperdrive_1578973612991526_1/model/generated_opinion_lex_reranked.csv already exists.\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAzureMLException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-7f449a5e23d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\azureml\\core\\run.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m                                      \"therefore, the {} cannot upload files, or log file backed metrics.\".format(\n\u001b[0;32m     48\u001b[0m                                          self, self.__class__.__name__))\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\azureml\\core\\run.py\u001b[0m in \u001b[0;36mupload_folder\u001b[1;34m(self, name, path)\u001b[0m\n\u001b[0;32m   1776\u001b[0m         \"\"\"\n\u001b[0;32m   1777\u001b[0m         return self._client.artifacts.upload_dir(path, RUN_ORIGIN, self._container,\n\u001b[1;32m-> 1778\u001b[1;33m                                                  lambda fpath: name + fpath[len(path):])\n\u001b[0m\u001b[0;32m   1779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1780\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0m_check_for_data_container_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\azureml\\_restclient\\artifacts_client.py\u001b[0m in \u001b[0;36mupload_dir\u001b[1;34m(self, dir_path, origin, container, path_to_name_fn)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Uploading {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths_to_upload\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\azureml\\_restclient\\artifacts_client.py\u001b[0m in \u001b[0;36mupload_files\u001b[1;34m(self, paths, origin, container, names, return_artifacts, timeout_seconds)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[0mbatch_paths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m                 \u001b[0mcontent_information\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_empty_artifacts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m                 \u001b[0martifacts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_information\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martifacts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\nlp\\lib\\site-packages\\azureml\\_restclient\\artifacts_client.py\u001b[0m in \u001b[0;36mcreate_empty_artifacts\u001b[1;34m(self, origin, container, paths)\u001b[0m\n\u001b[0;32m     72\u001b[0m                 error_messages.append(\"{}: {}\".format(error.code,\n\u001b[0;32m     73\u001b[0m                                                       error.message))\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAzureMLException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_messages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAzureMLException\u001b[0m: AzureMLException:\n\tMessage: UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.absa_hyperdrive_1578973612991526_1/model/generated_aspect_lex.csv already exists.\nUserError: Resource Conflict: ArtifactId ExperimentRun/dcid.absa_hyperdrive_1578973612991526_1/model/generated_opinion_lex_reranked.csv already exists.\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun/dcid.absa_hyperdrive_1578973612991526_1/model/generated_aspect_lex.csv already exists.\\nUserError: Resource Conflict: ArtifactId ExperimentRun/dcid.absa_hyperdrive_1578973612991526_1/model/generated_opinion_lex_reranked.csv already exists.\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "best_run.upload_folder(name=\"model\", path=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='absa', model_path='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model absa\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "\n",
    "model = Model.register(workspace=ws, model_name='absa', model_path='model', \n",
    "                      description='Aspect Based Sentiment Analysis - Intel',\n",
    "                      tags={'area': 'NLP', 'type': 'unsupervised', 'model_author': \"INTEL\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Locally\n",
    "\n",
    "### Install Local PIP Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/NervanaSystems/nlp-architect.git@absa   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy==2.0.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model From AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "from nlp_architect.models.absa.inference.inference import SentimentInference\n",
    "c_aspect_lex = 'outputs/generated_aspect_lex.csv'\n",
    "c_opinion_lex = 'outputs/generated_opinion_lex_reranked.csv' \n",
    "inference = SentimentInference(c_aspect_lex, c_opinion_lex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model On Sample Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0\n",
      "Batch 0 Done\n",
      "Processing batch 0\n",
      "Batch 0 Done\n",
      "Processing batch 0\n",
      "Batch 0 Done\n"
     ]
    }
   ],
   "source": [
    "docs = [\"Loved the sweater but hated the pants\",\n",
    "       \"Really great outfit, but the shirt is the wrong size\",\n",
    "       \"I absolutely love this jacket! i wear it almost everyday. works as a cardigan or a jacket. my favorite retailer purchase so far\"]\n",
    "\n",
    "sentiment_docs = []\n",
    "\n",
    "for doc_raw in docs:\n",
    "    sentiment_doc = inference.run(doc=doc_raw)\n",
    "    sentiment_docs.append(sentiment_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Loved the sweater but hated the \n",
       "<mark class=\"entity\" style=\"background: #FF0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    pants\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NEG</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Really great outfit, but the shirt is the wrong \n",
       "<mark class=\"entity\" style=\"background: #FF0000; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    size\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NEG</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I absolutely love this \n",
       "<mark class=\"entity\" style=\"background: #7CFC00; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    jacket\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">POS</span>\n",
       "</mark>\n",
       "! i wear it almost everyday. works as a cardigan or a jacket. my favorite \n",
       "<mark class=\"entity\" style=\"background: #7CFC00; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    retailer purchase\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">POS</span>\n",
       "</mark>\n",
       " so far</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from nlp_architect.models.absa.inference.data_types import TermType\n",
    "ents = []\n",
    "for doc in sentiment_docs:    \n",
    "    if doc:\n",
    "        doc_viz = {'text':doc._doc_text, 'ents':[]}\n",
    "        for s in doc._sentences:\n",
    "            for ev in s._events:\n",
    "                for e in ev:\n",
    "                    if e._type == TermType.ASPECT:\n",
    "                        ent = {'start': e._start, 'end': e._start + e._len,\n",
    "                               'label':str(e._polarity.value), \n",
    "                               'text':str(e._text)}\n",
    "                        if all(kown_e['start'] != ent['start'] for kown_e in ents):\n",
    "                            ents.append(ent)\n",
    "                            doc_viz['ents'].append(ent)\n",
    "        doc_viz['ents'].sort(key=lambda m: m[\"start\"])\n",
    "        displacy.render(doc_viz, style=\"ent\", options={'colors':{'POS':'#7CFC00', 'NEG':'#FF0000'}}, \n",
    "                        manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create configuration files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Enviorment File\n",
    "create an environment file, called myenv.yml, that specifies all of the script's package dependencies. This file is used to ensure that all of those dependencies are installed in the Docker image. This model needs nlp-architect and the azureml-sdk. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "pip = [\"azureml-defaults\", \"azureml-monitoring\", \n",
    "       \"git+https://github.com/NervanaSystems/nlp-architect.git@absa\", \n",
    "       \"spacy==2.0.18\",\n",
    "       \"\"]\n",
    "\n",
    "myenv = CondaDependencies.create(pip_packages=pip)\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment Config\n",
    "Create a Enviorment configuration file and specify the enviroment and enviormental variables required for the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "deploy_env = Environment.from_conda_specification('absa_env', \"myenv.yml\")\n",
    "deploy_env.environment_variables={'NLP_ARCHITECT_BE': 'CPU'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and Deployment Config \n",
    "Create an inference configuration that recieves the deployment enviorment and the entry script as well as a deployment configuration to run inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "inference_config = InferenceConfig(environment=deploy_env,\n",
    "                                   entry_script=\"score.py\")\n",
    "\n",
    "deploy_config = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1,\n",
    "                                               description='Aspect-Based Sentiment Analysis - Intel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Deploy!\n",
    "Create a deployment of the model using the scoring file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deployment = Model.deploy(ws, 'absa', \n",
    "                 models=[model], \n",
    "                 inference_config=inference_config, \n",
    "                 deployment_config=deploy_config, \n",
    "                 overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We now have gone through all the steps for production training of a custom open source model using the AzureML Service check out AIML50 to learn how to deploy and models and manage re-training pipelines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
