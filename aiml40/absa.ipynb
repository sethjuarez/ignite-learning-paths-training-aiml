{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copyfile, rmtree\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.core import Workspace, Datastore, Experiment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from nlp_architect.models.absa.inference.inference import SentimentInference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "To access an Azure ML Workspace, you will need to import the AML library and the following information:\n",
    "* A name for your workspace (in our example - `hal`)\n",
    "* Your subscription id (can be obtained by running `az account list`)\n",
    "* The resource group name (in our case `robots`)\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace/?WT.mc_id=absa-notebook-abornst) object from the existing workspace you created in the Prerequisites step or create a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hal\twestus2\trobots\twestus2\n",
      "Library configuration succeeded\n"
     ]
    }
   ],
   "source": [
    "#subscription_id = ''\n",
    "#resource_group  = 'hal'\n",
    "#workspace_name  = 'robots'\n",
    "#ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "#ws.write_config()\n",
    "\n",
    "try:\n",
    "    ws = Workspace.from_config()\n",
    "    print(ws.name, ws.location, ws.resource_group, ws.location, sep='\\t')\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two computer option run once(preview) and persistent compute for this demo we will use persistent compute to learn more about run once compute check out the [docs](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-set-up-training-targets#amlcompute?WT.mc_id=absa-notebook-abornst)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "# Choose a name for your CPU cluster\n",
    "cluster_name = \"gandalf\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\n",
    "                                                           vm_priority='lowpriority',\n",
    "                                                           min_nodes=1,\n",
    "                                                           max_nodes=4)\n",
    "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "    cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data\n",
    "\n",
    "The dataset we are using comes from the [womens ecommerce clothing reviews dataset](https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/) and is in the open domain, this can be replaced with any csv file with rows of text as the absa model is unsupervised. \n",
    "\n",
    "The documentation for uploading data can be found [here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.data.azure_storage_datastore.azureblobdatastore/?WT.mc_id=absa-notebook-abornst) for now we will us the ds.upload command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_root = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "ds = ws.get_default_datastore()\n",
    "ds.upload('./dataset', target_path='clothing_data', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Datastore Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Datastore.get(ws, 'absa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "\n",
    "Create an [Experiment](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiment/?WT.mc_id=absa-notebook-abornst) to track all the runs in your workspace for this distributed PyTorch tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'absa'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = {\n",
    "    '--data_folder': ds,\n",
    "    '--large': 'yes'\n",
    "}\n",
    "\n",
    "nlp_est = Estimator(source_directory='.',\n",
    "                   script_params=script_params,\n",
    "                   compute_target=cluster,\n",
    "                   environment_variables = {'NLP_ARCHITECT_BE':'CPU'},\n",
    "                   entry_script='train.py',\n",
    "                   pip_packages=['git+https://github.com/NervanaSystems/nlp-architect.git@absa',\n",
    "                                 'spacy==2.1.8']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a run we just submit our expierment as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(nlp_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you accidently run the following cell more than once you can cancel a run with the run.cancel() command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load any previous run using its run id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'absa_1579824592_b5671411'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = [r for r in exp.get_runs() if r.id == 'absa_1579824592_b5671411'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3ff0f4959a49548951e9be4ea0ca97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Running\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/absa/runs/absa_1579824592_b5671411?wsid=/subscriptions/91d27443-f037-45d9-bb0c-428256992df6/resourcegroups/robots/workspaces/hal\", \"run_id\": \"absa_1579824592_b5671411\", \"run_properties\": {\"run_id\": \"absa_1579824592_b5671411\", \"created_utc\": \"2020-01-24T00:10:09.086698Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"amlcompute\", \"ContentSnapshotId\": \"6c0356cf-90dc-46e8-af7d-372ccfa4d18d\", \"azureml.git.repository_uri\": \"git@github.com:sethjuarez/ignite-learning-paths-training-aiml.git\", \"mlflow.source.git.repoURL\": \"git@github.com:sethjuarez/ignite-learning-paths-training-aiml.git\", \"azureml.git.branch\": \"master\", \"mlflow.source.git.branch\": \"master\", \"azureml.git.commit\": \"b039491a8aaceff5fde02e554af52885b95e71bc\", \"mlflow.source.git.commit\": \"b039491a8aaceff5fde02e554af52885b95e71bc\", \"azureml.git.dirty\": \"True\", \"AzureML.DerivedImageName\": \"azureml/azureml_4a5982480b0c46c895f60f0781940af2\", \"ProcessInfoFile\": \"azureml-logs/process_info.json\", \"ProcessStatusFile\": \"azureml-logs/process_status.json\"}, \"tags\": {\"_aml_system_ComputeTargetStatus\": \"{\\\"AllocationState\\\":\\\"steady\\\",\\\"PreparingNodeCount\\\":0,\\\"RunningNodeCount\\\":1,\\\"CurrentNodeCount\\\":1}\"}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Running\", \"log_files\": {\"azureml-logs/55_azureml-execution-tvmps_50ce83559f2a9c10db8d1fa3bc8557c30f312580df5e251f0a0ce02cec76ddbf_d.txt\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/azureml-logs/55_azureml-execution-tvmps_50ce83559f2a9c10db8d1fa3bc8557c30f312580df5e251f0a0ce02cec76ddbf_d.txt?sv=2019-02-02&sr=b&sig=IfKH3cCgSvTqbk5NPHCHROSZUa%2B9q9rlSoDsC69Z5uo%3D&st=2020-01-24T00%3A08%3A50Z&se=2020-01-24T08%3A18%3A50Z&sp=r\", \"azureml-logs/65_job_prep-tvmps_50ce83559f2a9c10db8d1fa3bc8557c30f312580df5e251f0a0ce02cec76ddbf_d.txt\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/azureml-logs/65_job_prep-tvmps_50ce83559f2a9c10db8d1fa3bc8557c30f312580df5e251f0a0ce02cec76ddbf_d.txt?sv=2019-02-02&sr=b&sig=40VGouHGM2VitnWhiogqcCKB0jMADo9if4vC4eAxlFw%3D&st=2020-01-24T00%3A08%3A50Z&se=2020-01-24T08%3A18%3A50Z&sp=r\", \"azureml-logs/70_driver_log.txt\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=pzO1UynwgcnQV5%2B290wnAl7bNhjt6NaYDXKLcFpikYw%3D&st=2020-01-24T00%3A08%3A50Z&se=2020-01-24T08%3A18%3A50Z&sp=r\", \"azureml-logs/process_info.json\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=LfZplkvHTCpKQaCwqn2FpK3O5T3KpTN4pt%2BPmxG6%2Fr8%3D&st=2020-01-24T00%3A08%3A50Z&se=2020-01-24T08%3A18%3A50Z&sp=r\", \"azureml-logs/process_status.json\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=Gs1p1xYBig0Xh%2B6D7tGJpf1%2FVDx40MCwZjRmiV7%2Fz%2FI%3D&st=2020-01-24T00%3A08%3A50Z&se=2020-01-24T08%3A18%3A50Z&sp=r\", \"logs/azureml/152_azureml.log\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/logs/azureml/152_azureml.log?sv=2019-02-02&sr=b&sig=aNSNcZPE9M1bb6wNfqCGy72bJVMH6outkq%2FUdx9h1AM%3D&st=2020-01-24T00%3A09%3A11Z&se=2020-01-24T08%3A19%3A11Z&sp=r\", \"logs/azureml/153_azureml.log\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/logs/azureml/153_azureml.log?sv=2019-02-02&sr=b&sig=tr7Aj%2BFPRQ6cvM3AXq%2F43%2BE55JFUzTzeVfAm%2B44QWIs%3D&st=2020-01-24T00%3A09%3A11Z&se=2020-01-24T08%3A19%3A11Z&sp=r\", \"logs/azureml/job_prep_azureml.log\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=4n0%2B8iQPR5qXHLAhADG8ZferNodz1EnA1Ia5cmDstsE%3D&st=2020-01-24T00%3A09%3A11Z&se=2020-01-24T08%3A19%3A11Z&sp=r\", \"logs/azureml/job_release_azureml.log\": \"https://halworkspacestorage.blob.core.windows.net/azureml/ExperimentRun/dcid.absa_1579824592_b5671411/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=c1dh2%2FslU8cMs8LPyBTY97PZu8a3G3Z2008B8n1xrvQ%3D&st=2020-01-24T00%3A09%3A11Z&se=2020-01-24T08%3A19%3A11Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/process_info.json\", \"azureml-logs/process_status.json\", \"logs/azureml/job_prep_azureml.log\", \"logs/azureml/job_release_azureml.log\"], [\"azureml-logs/55_azureml-execution-tvmps_50ce83559f2a9c10db8d1fa3bc8557c30f312580df5e251f0a0ce02cec76ddbf_d.txt\"], [\"azureml-logs/65_job_prep-tvmps_50ce83559f2a9c10db8d1fa3bc8557c30f312580df5e251f0a0ce02cec76ddbf_d.txt\"], [\"azureml-logs/70_driver_log.txt\"], [\"logs/azureml/152_azureml.log\"], [\"logs/azureml/153_azureml.log\"]], \"run_duration\": \"0:09:01\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"bash: /azureml-envs/azureml_c8b4e2a76a42a52100edbd4e5e8fc9cf/lib/libtinfo.so.5: no version information available (required by bash)\\nbash: /azureml-envs/azureml_c8b4e2a76a42a52100edbd4e5e8fc9cf/lib/libtinfo.so.5: no version information available (required by bash)\\nStarting the daemon thread to refresh tokens in background for process with pid = 152\\nEntering Run History Context Manager.\\nCollecting en_core_web_sm==2.1.0\\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1MB)\\nBuilding wheels for collected packages: en-core-web-sm\\n  Building wheel for en-core-web-sm (setup.py): started\\n  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\\n  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.1.0-cp36-none-any.whl size=11074435 sha256=ea0c91e82622a42a03022a1ac6d3678b20f582e16ad4d45637fd6742a7b21ad7\\n  Stored in directory: /tmp/pip-ephem-wheel-cache-x9unlqp1/wheels/39/ea/3b/507f7df78be8631a7a3d7090962194cf55bc1158572c0be77f\\nSuccessfully built en-core-web-sm\\nInstalling collected packages: en-core-web-sm\\nSuccessfully installed en-core-web-sm-2.1.0\\n\\u001b[38;5;2m\\u2714 Download and installation successful\\u001b[0m\\nYou can now load the model via spacy.load('en_core_web_sm')\\n\\u001b[38;5;2m\\u2714 Linking successful\\u001b[0m\\n/azureml-envs/azureml_c8b4e2a76a42a52100edbd4e5e8fc9cf/lib/python3.6/site-packages/en_core_web_sm\\n-->\\n/azureml-envs/azureml_c8b4e2a76a42a52100edbd4e5e8fc9cf/lib/python3.6/site-packages/spacy/data/en\\nYou can now load the model via spacy.load('en')\\nUsing large dataset: clothing_data/clothing_absa_train.csv\\nusing pre-trained reranking model\\ndowloading pre-trained reranking model..\\nDownloading file to: /root/nlp-architect/cache/absa/train/reranking_model/rerank_model.h5\\n\\r0MB [00:00, ?MB/s]\\r1MB [00:00, 21.46MB/s]\\nDownload Complete\\nProcessing batch 0\\nProcessing batch 1\\nProcessing batch 2\\nProcessing batch 3\\nProcessing batch 4\\nProcessing batch 5\\nProcessing batch 6\\nProcessing batch 7\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.83\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.download_files()\n",
    "\n",
    "if os.path.exists('model'):\n",
    "    rmtree('model')\n",
    "    \n",
    "os.makedirs('model')\n",
    "\n",
    "aspect_lex = copyfile('outputs/generated_aspect_lex.csv', 'model/generated_aspect_lex.csv')\n",
    "opinion_lex = copyfile('outputs/generated_opinion_lex_reranked.csv', 'model/generated_opinion_lex_reranked.csv')\n",
    "\n",
    "run.upload_folder(name='model_files', path='model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='absa', model_path='modelfiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model On Sample Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_aspect_lex = 'model/generated_aspect_lex.csv'\n",
    "c_opinion_lex = 'model/generated_opinion_lex_reranked.csv' \n",
    "inference = SentimentInference(c_aspect_lex, c_opinion_lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"Loved the sweater but hated the pants\",\n",
    "       \"Really great outfit, but the shirt is the wrong size\",\n",
    "       \"I absolutely love this jacket! i wear it almost everyday. works as a cardigan or a jacket. my favorite retailer purchase so far\"]\n",
    "\n",
    "sentiment_docs = []\n",
    "\n",
    "for doc_raw in docs:\n",
    "    sentiment_doc = inference.run(doc=doc_raw)\n",
    "    sentiment_docs.append(sentiment_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from nlp_architect.models.absa.inference.data_types import TermType\n",
    "ents = []\n",
    "for doc in sentiment_docs:    \n",
    "    if doc:\n",
    "        doc_viz = {'text':doc._doc_text, 'ents':[]}\n",
    "        for s in doc._sentences:\n",
    "            for ev in s._events:\n",
    "                for e in ev:\n",
    "                    if e._type == TermType.ASPECT:\n",
    "                        ent = {'start': e._start, 'end': e._start + e._len,\n",
    "                               'label':str(e._polarity.value), \n",
    "                               'text':str(e._text)}\n",
    "                        if all(kown_e['start'] != ent['start'] for kown_e in ents):\n",
    "                            ents.append(ent)\n",
    "                            doc_viz['ents'].append(ent)\n",
    "        doc_viz['ents'].sort(key=lambda m: m[\"start\"])\n",
    "        displacy.render(doc_viz, style=\"ent\", options={'colors':{'POS':'#7CFC00', 'NEG':'#FF0000'}}, \n",
    "                        manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We now have gone through all the steps for production training of a custom open source model using the AzureML Service check out AIML50 to learn how to deploy and models and manage re-training pipelines."
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
